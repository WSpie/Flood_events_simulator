{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os, sys\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from tqdm.auto import tqdm, trange\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import re\n",
    "import concurrent.futures\n",
    "from sklearn.metrics import silhouette_score\n",
    "from scipy.stats import pearsonr\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from kmeans_pytorch import kmeans\n",
    "\n",
    "from sdv.metadata import SingleTableMetadata\n",
    "from sdv.evaluation.single_table import evaluate_quality\n",
    "from sdv.single_table import CTGANSynthesizer\n",
    "from sdv.sampling import Condition\n",
    "from sdv.evaluation.single_table import get_column_plot\n",
    "\n",
    "import dask.dataframe as dpd\n",
    "import dask_geopandas as dgpd\n",
    "from dask.diagnostics import ProgressBar\n",
    "from dask.distributed import Client\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(n_workers=128) #192 totally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.2+cu121 True\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__, torch.cuda.is_available())\n",
    "torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 8 GPUs!\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"We have {torch.cuda.device_count()} GPUs!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot geo map\n",
    "def plot_map(gdf, col, vmin=0, vmax=300, figsize=(8, 6), dpi=200, notes='', to_path='', dots=[], title=True, s=1):\n",
    "    plt.clf()\n",
    "    fig, ax = plt.subplots(figsize=figsize, dpi=dpi)\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.1)\n",
    "\n",
    "    # Plot without specifying legend_kwds\n",
    "    try:\n",
    "        gdf.plot(ax=ax, column=col, cmap='coolwarm', vmin=vmin, vmax=vmax, cax=cax, s=s)\n",
    "    except:\n",
    "        gdf.plot(ax=ax, column=col, cmap='coolwarm', vmin=vmin, vmax=vmax, cax=cax)\n",
    "    if dots:\n",
    "        for dot in dots:\n",
    "            gdf.loc[dot:dot].plot(ax=ax, linewidth=1, color='black', alpha=0.5)\n",
    "            ax.text(gdf.loc[dot, 'x'], gdf.loc[dot, 'y'], str(dot), fontsize=12)\n",
    "\n",
    "    # Create colorbar with custom font size\n",
    "    sm = plt.cm.ScalarMappable(cmap='coolwarm', norm=plt.Normalize(vmin=vmin, vmax=vmax))\n",
    "    sm.set_array([])\n",
    "    cbar = fig.colorbar(sm, cax=cax, \n",
    "#                         label=col.upper(), \n",
    "                        shrink=.5)\n",
    "#     cbar.ax.tick_params(labelsize=20)  # Set the font size for the colorbar\n",
    "#     bd_gdf.boundary.plot(ax=ax, linewidth=1, color='k')\n",
    "    # Change tick fontsize\n",
    "    ax.tick_params(axis='both', which='major', labelsize=20)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "#     ax.scatter(gdf['x'], gdf['y'], s=1, c='k')\n",
    "    \n",
    "    # Change color bar fontsize\n",
    "#     cbar.set_label(col.upper(), fontsize=20)\n",
    "    if title:\n",
    "        ax.set_title(f'{col.upper()}')\n",
    "#     if not os.path.exists(f'plots/test/{notes}'):\n",
    "#         os.mkdir(f'plots/test/{notes}')\n",
    "#     fig.savefig(f'plots/test/{notes}/{col}.png')\n",
    "    if to_path:\n",
    "        fig.savefig(f'{to_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m coord_gdf \u001b[38;5;241m=\u001b[39m \u001b[43mgpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_file\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../src/coord/coord_gdf.shp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m coord_gdf \u001b[38;5;241m=\u001b[39m coord_gdf\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcell_rmse1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcell_r21\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcell_rmse2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcell_r22\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdepth\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      3\u001b[0m coord_gdf\n",
      "File \u001b[0;32m~/Pie_projects/GAN_simulated_events/se_env/lib/python3.8/site-packages/geopandas/io/file.py:281\u001b[0m, in \u001b[0;36m_read_file\u001b[0;34m(filename, bbox, mask, rows, engine, **kwargs)\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    279\u001b[0m         path_or_bytes \u001b[38;5;241m=\u001b[39m filename\n\u001b[0;32m--> 281\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read_file_fiona\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_bytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_bytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbbox\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbbox\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munknown engine \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mengine\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Pie_projects/GAN_simulated_events/se_env/lib/python3.8/site-packages/geopandas/io/file.py:379\u001b[0m, in \u001b[0;36m_read_file_fiona\u001b[0;34m(path_or_bytes, from_bytes, bbox, mask, rows, where, **kwargs)\u001b[0m\n\u001b[1;32m    375\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\n\u001b[1;32m    376\u001b[0m         [record[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproperties\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m record \u001b[38;5;129;01min\u001b[39;00m f_filt], columns\u001b[38;5;241m=\u001b[39mcolumns\n\u001b[1;32m    377\u001b[0m     )\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 379\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mGeoDataFrame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_features\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[43m        \u001b[49m\u001b[43mf_filt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgeometry\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m datetime_fields:\n\u001b[1;32m    383\u001b[0m     as_dt \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(df[k], errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Pie_projects/GAN_simulated_events/se_env/lib/python3.8/site-packages/geopandas/geodataframe.py:638\u001b[0m, in \u001b[0;36mGeoDataFrame.from_features\u001b[0;34m(cls, features, crs, columns)\u001b[0m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m features_lst:\n\u001b[1;32m    636\u001b[0m     \u001b[38;5;66;03m# load geometry\u001b[39;00m\n\u001b[1;32m    637\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(feature, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__geo_interface__\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 638\u001b[0m         feature \u001b[38;5;241m=\u001b[39m \u001b[43mfeature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__geo_interface__\u001b[49m\n\u001b[1;32m    639\u001b[0m     row \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    640\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeometry\u001b[39m\u001b[38;5;124m\"\u001b[39m: shape(feature[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeometry\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;28;01mif\u001b[39;00m feature[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeometry\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    641\u001b[0m     }\n\u001b[1;32m    642\u001b[0m     \u001b[38;5;66;03m# load properties\u001b[39;00m\n",
      "File \u001b[0;32m~/Pie_projects/GAN_simulated_events/se_env/lib/python3.8/site-packages/fiona/model.py:367\u001b[0m, in \u001b[0;36mFeature.__geo_interface__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__geo_interface__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 367\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mObjectEncoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefault\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Pie_projects/GAN_simulated_events/se_env/lib/python3.8/site-packages/fiona/model.py:388\u001b[0m, in \u001b[0;36mObjectEncoder.default\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault\u001b[39m(\u001b[38;5;28mself\u001b[39m, o):\n\u001b[1;32m    387\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(o, Object):\n\u001b[0;32m--> 388\u001b[0m         o_dict \u001b[38;5;241m=\u001b[39m {k: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault(v) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m o\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m    389\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(o, Geometry):\n\u001b[1;32m    390\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m o\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGeometryCollection\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/Pie_projects/GAN_simulated_events/se_env/lib/python3.8/site-packages/fiona/model.py:388\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault\u001b[39m(\u001b[38;5;28mself\u001b[39m, o):\n\u001b[1;32m    387\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(o, Object):\n\u001b[0;32m--> 388\u001b[0m         o_dict \u001b[38;5;241m=\u001b[39m {k: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefault\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m o\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m    389\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(o, Geometry):\n\u001b[1;32m    390\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m o\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGeometryCollection\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/Pie_projects/GAN_simulated_events/se_env/lib/python3.8/site-packages/fiona/model.py:388\u001b[0m, in \u001b[0;36mObjectEncoder.default\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault\u001b[39m(\u001b[38;5;28mself\u001b[39m, o):\n\u001b[1;32m    387\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(o, Object):\n\u001b[0;32m--> 388\u001b[0m         o_dict \u001b[38;5;241m=\u001b[39m {k: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault(v) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m o\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m    389\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(o, Geometry):\n\u001b[1;32m    390\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m o\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGeometryCollection\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/Pie_projects/GAN_simulated_events/se_env/lib/python3.8/site-packages/fiona/model.py:388\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault\u001b[39m(\u001b[38;5;28mself\u001b[39m, o):\n\u001b[1;32m    387\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(o, Object):\n\u001b[0;32m--> 388\u001b[0m         o_dict \u001b[38;5;241m=\u001b[39m {k: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefault\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m o\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m    389\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(o, Geometry):\n\u001b[1;32m    390\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m o\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGeometryCollection\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/Pie_projects/GAN_simulated_events/se_env/lib/python3.8/site-packages/fiona/model.py:397\u001b[0m, in \u001b[0;36mObjectEncoder.default\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    395\u001b[0m         o_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m o_dict\n\u001b[0;32m--> 397\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28;43misinstance\u001b[39;49m(o, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[1;32m    398\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hexlify(o)\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "coord_gdf = gpd.read_file('../src/coord/coord_gdf.shp')\n",
    "coord_gdf = coord_gdf.drop(columns=['cell_rmse1', 'cell_r21', 'cell_rmse2', 'cell_r22', 'depth'])\n",
    "coord_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Cypress Creek': 0,\n",
       " 'Greens Bayou': 1,\n",
       " 'Whiteoak Bayou-Buffalo Bayou': 2,\n",
       " 'Addicks Reservoir': 3,\n",
       " 'Barker Reservoir': 4,\n",
       " 'Hunting Bayou': 5,\n",
       " 'Vince Bayou-Buffalo Bayou': 6,\n",
       " 'Brays Bayou': 7,\n",
       " 'Sims Bayou': 8}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "watershed_dict = coord_gdf.drop_duplicates(subset='HUC12').set_index('HUC12')['region'].to_dict()\n",
    "watershed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "xy_scaled = scaler.fit_transform(coord_gdf[['x', 'y']])\n",
    "\n",
    "def load_and_scale(file_path, scale=False):\n",
    "    df = pd.read_parquet(file_path)[['x', 'y', 'channel', 'ter', 'cumu_rain', 'peak_int', 'duration', 'depth']]\n",
    "    if scale:\n",
    "        df[['x', 'y']] = xy_scaled\n",
    "    return df\n",
    "\n",
    "file_paths = [f'../src/tables/data{i}.parquet' for i in range(1, len([f for f in os.listdir('../src/tables') if f.endswith('.parquet')]) + 1)]\n",
    "events = [load_and_scale(file) for file in file_paths]\n",
    "with ProgressBar():\n",
    "    result = dpd.concat(events, axis=0)\n",
    "events_df = result.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>channel</th>\n",
       "      <th>ter</th>\n",
       "      <th>cumu_rain</th>\n",
       "      <th>peak_int</th>\n",
       "      <th>duration</th>\n",
       "      <th>depth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.933766e+06</td>\n",
       "      <td>1.396557e+07</td>\n",
       "      <td>0</td>\n",
       "      <td>301.388702</td>\n",
       "      <td>3.001601</td>\n",
       "      <td>3.001601</td>\n",
       "      <td>1</td>\n",
       "      <td>3.866364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.934966e+06</td>\n",
       "      <td>1.396557e+07</td>\n",
       "      <td>0</td>\n",
       "      <td>301.594696</td>\n",
       "      <td>3.127318</td>\n",
       "      <td>3.127318</td>\n",
       "      <td>1</td>\n",
       "      <td>2.150513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.933766e+06</td>\n",
       "      <td>1.396437e+07</td>\n",
       "      <td>0</td>\n",
       "      <td>294.629181</td>\n",
       "      <td>3.211096</td>\n",
       "      <td>3.211096</td>\n",
       "      <td>1</td>\n",
       "      <td>3.595856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.934966e+06</td>\n",
       "      <td>1.396437e+07</td>\n",
       "      <td>0</td>\n",
       "      <td>298.529877</td>\n",
       "      <td>3.260372</td>\n",
       "      <td>3.260372</td>\n",
       "      <td>1</td>\n",
       "      <td>2.782227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.936166e+06</td>\n",
       "      <td>1.396437e+07</td>\n",
       "      <td>0</td>\n",
       "      <td>294.815002</td>\n",
       "      <td>3.309647</td>\n",
       "      <td>3.309647</td>\n",
       "      <td>1</td>\n",
       "      <td>2.787598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26296</th>\n",
       "      <td>3.039069e+06</td>\n",
       "      <td>1.385008e+07</td>\n",
       "      <td>1</td>\n",
       "      <td>54.643570</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26297</th>\n",
       "      <td>3.039053e+06</td>\n",
       "      <td>1.385088e+07</td>\n",
       "      <td>1</td>\n",
       "      <td>59.625050</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26298</th>\n",
       "      <td>3.038396e+06</td>\n",
       "      <td>1.385006e+07</td>\n",
       "      <td>0</td>\n",
       "      <td>60.055576</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26299</th>\n",
       "      <td>3.038392e+06</td>\n",
       "      <td>1.385087e+07</td>\n",
       "      <td>0</td>\n",
       "      <td>59.625050</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26300</th>\n",
       "      <td>3.037707e+06</td>\n",
       "      <td>1.385085e+07</td>\n",
       "      <td>0</td>\n",
       "      <td>72.228638</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15570192 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  x             y  channel         ter  cumu_rain  peak_int  \\\n",
       "0      2.933766e+06  1.396557e+07        0  301.388702   3.001601  3.001601   \n",
       "1      2.934966e+06  1.396557e+07        0  301.594696   3.127318  3.127318   \n",
       "2      2.933766e+06  1.396437e+07        0  294.629181   3.211096  3.211096   \n",
       "3      2.934966e+06  1.396437e+07        0  298.529877   3.260372  3.260372   \n",
       "4      2.936166e+06  1.396437e+07        0  294.815002   3.309647  3.309647   \n",
       "...             ...           ...      ...         ...        ...       ...   \n",
       "26296  3.039069e+06  1.385008e+07        1   54.643570   0.000000  0.000000   \n",
       "26297  3.039053e+06  1.385088e+07        1   59.625050   0.000000  0.000000   \n",
       "26298  3.038396e+06  1.385006e+07        0   60.055576   0.000000  0.000000   \n",
       "26299  3.038392e+06  1.385087e+07        0   59.625050   0.000000  0.000000   \n",
       "26300  3.037707e+06  1.385085e+07        0   72.228638   0.000000  0.000000   \n",
       "\n",
       "       duration     depth  \n",
       "0             1  3.866364  \n",
       "1             1  2.150513  \n",
       "2             1  3.595856  \n",
       "3             1  2.782227  \n",
       "4             1  2.787598  \n",
       "...         ...       ...  \n",
       "26296         2  0.000000  \n",
       "26297         2  0.000000  \n",
       "26298         2  0.000000  \n",
       "26299         2  0.000000  \n",
       "26300         2  0.000000  \n",
       "\n",
       "[15570192 rows x 8 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "events_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = events_df[['channel', 'ter', 'cumu_rain', 'peak_int', 'duration']].values\n",
    "targets = events_df[['depth']].values\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    features, targets, test_size=0.4, random_state=0)\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=0)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DepthModelDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 40960\n",
    "\n",
    "train_dataset = DepthModelDataset(X_train_scaled, y_train)\n",
    "val_dataset = DepthModelDataset(X_val_scaled, y_val)\n",
    "test_dataset = DepthModelDataset(X_test_scaled, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=128, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=128, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=128, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim=1, num_heads=8, num_layers=4, transform_dim=128):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.input_transform = nn.Linear(input_dim, transform_dim)\n",
    "\n",
    "        # Transformer specific layers\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=transform_dim, \n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=2048,  \n",
    "            dropout=0.25  \n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            self.encoder_layer, \n",
    "            num_layers=num_layers\n",
    "        )\n",
    "\n",
    "        # Output layer\n",
    "        self.fc_out = nn.Linear(transform_dim, output_dim)\n",
    "\n",
    "    def forward(self, src):\n",
    "        src = self.input_transform(src)\n",
    "        transformed = self.transformer_encoder(src)\n",
    "        output = self.fc_out(transformed)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = TransformerModel(train_dataset.features.shape[1]).to(device)\n",
    "model = nn.DataParallel(model)\n",
    "\n",
    "print(next(model.parameters()).is_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-4\n",
    "num_epochs = 500\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "history = {'train_loss': [], 'val_loss': []}\n",
    "best_val_loss = float('inf')\n",
    "best_model_wts = copy.deepcopy(model.state_dict())\n",
    "checkpoint_path = os.path.join('../checkpoints', 'depth', 'best_model.pth')\n",
    "depth_checkpoint_log_path = '../logs/depth.log'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/500 [01:52<15:36:24, 112.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to ../checkpoints/depth/best_model.pth with val loss 11.2213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _releaseLock at 0x7f556afb41f0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/logging/__init__.py\", line 227, in _releaseLock\n",
      "    def _releaseLock():\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "for epoch in trange(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for batch_idx, (features, labels) in enumerate(train_loader):\n",
    "        features = features.to(device).float()\n",
    "        labels = labels.to(device).float()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(features)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for features, labels in val_loader:\n",
    "            features = features.to(device).float()\n",
    "            labels = labels.to(device).float()\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    # Average losses\n",
    "    train_loss /= len(train_loader)\n",
    "    val_loss /= len(val_loader)\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "\n",
    "    # Save model if validation loss has improved\n",
    "    if val_loss < best_val_loss:\n",
    "        depth_checkpoint_log_file = open(depth_checkpoint_log_path, 'a+')\n",
    "        best_val_loss = val_loss\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        torch.save(model.state_dict(), checkpoint_path)\n",
    "        print(f'Epoch {epoch}: Saved model to {checkpoint_path} with val loss {val_loss:.4f}', file=depth_checkpoint_log_file)\n",
    "        depth_checkpoint_log_file.flush()\n",
    "        depth_checkpoint_log_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "se_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
